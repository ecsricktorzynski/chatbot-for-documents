{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JUYIIBrTw9R"
   },
   "source": [
    "# Custom Knowledge Chatbot w/ LlamaIndex\n",
    "By Liam Ottley\n",
    "\n",
    "YouTube: https://www.youtube.com/@LiamOttley\n",
    "\n",
    "Github: https://github.com/wombyz/custom-knowledge-chatbot/blob/main/custom-knowledge-chatbot/Custom%20Knowledge%20Chatbot.ipynb\n",
    "\n",
    "LlamaIndex: https://gpt-index.readthedocs.io/en/latest/index.html\n",
    "\n",
    "## Customization\n",
    "Using the example above with fixes from LlamaIndex documentation, this notebook will index the documents in the data directory and allow you to query the index through a chatbot interface\n",
    "\n",
    "## Installation\n",
    "*   Create data directory\n",
    "*   Upload documents to data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FmfuhnUdT-PM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: llama_index in ./.venv/lib/python3.8/site-packages (0.5.5)\n",
      "Requirement already satisfied: dataclasses_json in ./.venv/lib/python3.8/site-packages (from llama_index) (0.5.7)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.8/site-packages (from llama_index) (0.0.129)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from llama_index) (1.23.5)\n",
      "Requirement already satisfied: openai>=0.26.4 in ./.venv/lib/python3.8/site-packages (from llama_index) (0.27.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (from llama_index) (1.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in ./.venv/lib/python3.8/site-packages (from llama_index) (8.2.2)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.8/site-packages (from llama_index) (4.27.4)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.8/site-packages (from openai>=0.26.4->llama_index) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.8/site-packages (from openai>=0.26.4->llama_index) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.8/site-packages (from openai>=0.26.4->llama_index) (2.28.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.venv/lib/python3.8/site-packages (from dataclasses_json->llama_index) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.venv/lib/python3.8/site-packages (from dataclasses_json->llama_index) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.venv/lib/python3.8/site-packages (from dataclasses_json->llama_index) (3.19.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain->llama_index) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.8/site-packages (from langchain->llama_index) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain->llama_index) (1.4.47)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas->llama_index) (2023.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (23.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (3.10.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (2023.3.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (0.13.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->llama_index) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (1.26.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.8/site-packages (from SQLAlchemy<2,>=1->langchain->llama_index) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.8/site-packages (from typing-inspect>=0.4.0->dataclasses_json->llama_index) (1.0.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.8/site-packages (0.0.129)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.8/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.8/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.venv/lib/python3.8/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.8/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.venv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.venv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.venv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.8/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.8/site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.8/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./.venv/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.8/site-packages (from nltk) (2023.3.23)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.8/site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index\n",
    "!pip install langchain\n",
    "!pip install nltk\n",
    "# install dotenv to read api_key from .env file\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuX_I816UT6q",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jmCP_UhPUzqj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 143439 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "from llama_index import GPTSimpleVectorIndex, download_loader, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhizT3ToZdm9",
    "outputId": "2481d14e-9c31-4b54-ae86-a194f04f749c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4027 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Some of the sparks of AGI that large language models are exhibiting include the ability to learn from large amounts of data, the ability to reason, plan, and create, the ability to exhibit general and flexible intelligence, the ability to learn generic and useful neural circuits, the ability to make gradient descent more effective, the ability to fit high-dimensional data, the ability to solve quantitative reasoning problems, the ability to learn shortcuts to automata, the ability to form scientific hypotheses, and the ability to develop autonomous machine intelligence.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What are some of the sparks of AGI that large language models are exhibiting?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-Gk9MNS0a1UJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rick/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 147176 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# Setup your LLM\n",
    "\n",
    "from llama_index import (\n",
    "    GPTKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ")\n",
    "from langchain import OpenAI\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "# build index\n",
    "index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pagpwrcMdtsC",
    "outputId": "db08e7b8-544e-4d26-ac01-6dc8a75045aa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.query:> Starting query: What are some of the sparks of AGI that large language models are exhibiting?\n",
      "INFO:llama_index.indices.keyword_table.query:query keywords: ['agi', 'sparks', 'models', 'large language models', 'language', 'large']\n",
      "INFO:llama_index.indices.keyword_table.query:> Extracted keywords: ['agi', 'language']\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 21765 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Some of the sparks of AGI that large language models are exhibiting are the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. Additionally, large language models are also displaying an ability to generalize from known data to new situations and data, as well as the ability to create new and original ideas.\n",
      "\n",
      "However, it is important to be aware of the potential dangers of these models, such as the spread of misinformation, propaganda, or hate speech. We must work to educate ourselves about the risks involved in using these models, and be vigilant in our efforts to mitigate the potential for harm.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What are some of the sparks of AGI that large language models are exhibiting?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_abcteoegRb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "index.save_to_disk('./index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPxoDL0Xg5Az",
    "outputId": "d09b5819-43be-4491-cf0f-438b80b591b9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.query:> Starting query: Summarize the Sparks of AGI paper\n",
      "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n",
      "INFO:llama_index.indices.keyword_table.query:query keywords: ['agi', 'physics', 'science', 'philosophy', 'general', 'mathematics', 'neuroscience', 'intelligence', 'cognitive', 'computer', 'artificial', 'cognitive science', 'superintelligence', 'artificial general intelligence', 'computer science']\n",
      "INFO:llama_index.indices.keyword_table.query:> Extracted keywords: ['agi', 'general', 'mathematics', 'intelligence', 'cognitive', 'computer', 'artificial', 'artificial general intelligence']\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 43945 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The paper explores the potential of GPT-4 to attain a form of general intelligence, demonstrating its core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform. While the model still has some limitations, the authors believe that it shows great promise for future development of artificial general intelligence.\n",
      "\n",
      "In addition, the paper discusses the fundamental questions of why and how GPT-4 achieves such remarkable intelligence. It is suggested that further research is needed to understand the capabilities of LLMs, in order to make progress in the development of artificial general intelligence.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Summarize the Sparks of AGI paper\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f8zzUchDlK4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import openai\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-rhB5UslQj7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is AGI?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.query:> Starting query: What is AGI?\n",
      "INFO:llama_index.indices.keyword_table.query:query keywords: ['agi', 'intelligence', 'general', 'artificial', 'artificial general intelligence']\n",
      "INFO:llama_index.indices.keyword_table.query:> Extracted keywords: ['agi', 'intelligence', 'general', 'artificial', 'artificial general intelligence']\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 28961 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n",
      "\n",
      "AGI stands for artificial general intelligence. It is a type of AI that is able to perform any task that a human can. AGI is different from other types of AI because it is not limited to one specific task.\n",
      "\n",
      "AGI is different from other types of AI because it is not limited to one specific task. It can learn new tasks over time, and eventually come to a halt as it masters all tasks.\n"
     ]
    }
   ],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(os.environ['OPENAI_API_KEY'], index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
