{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JUYIIBrTw9R"
   },
   "source": [
    "# Custom Knowledge Chatbot w/ LlamaIndex\n",
    "By Liam Ottley\n",
    "\n",
    "YouTube: https://www.youtube.com/@LiamOttley\n",
    "\n",
    "Github: https://github.com/wombyz/custom-knowledge-chatbot/blob/main/custom-knowledge-chatbot/Custom%20Knowledge%20Chatbot.ipynb\n",
    "\n",
    "LlamaIndex: https://gpt-index.readthedocs.io/en/latest/index.html\n",
    "\n",
    "## Customization\n",
    "Using the example above with fixes from LlamaIndex documentation, this notebook will index the documents in the data directory and allow you to query the index through a chatbot interface\n",
    "\n",
    "## Installation\n",
    "*   Create data directory\n",
    "*   Upload documents to data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FmfuhnUdT-PM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in ./.venv/lib/python3.8/site-packages (0.5.5)\n",
      "Requirement already satisfied: dataclasses_json in ./.venv/lib/python3.8/site-packages (from llama_index) (0.5.7)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.8/site-packages (from llama_index) (0.0.129)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from llama_index) (1.23.5)\n",
      "Requirement already satisfied: openai>=0.26.4 in ./.venv/lib/python3.8/site-packages (from llama_index) (0.27.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (from llama_index) (1.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in ./.venv/lib/python3.8/site-packages (from llama_index) (8.2.2)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.8/site-packages (from llama_index) (4.27.4)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.8/site-packages (from openai>=0.26.4->llama_index) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.8/site-packages (from openai>=0.26.4->llama_index) (2.28.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.8/site-packages (from openai>=0.26.4->llama_index) (4.65.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.venv/lib/python3.8/site-packages (from dataclasses_json->llama_index) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.venv/lib/python3.8/site-packages (from dataclasses_json->llama_index) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.venv/lib/python3.8/site-packages (from dataclasses_json->llama_index) (3.19.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain->llama_index) (1.4.47)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain->llama_index) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.8/site-packages (from langchain->llama_index) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.8/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas->llama_index) (2023.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (23.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from transformers->llama_index) (3.10.7)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->openai>=0.26.4->llama_index) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->llama_index) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.8/site-packages (from SQLAlchemy<2,>=1->langchain->llama_index) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.8/site-packages (from typing-inspect>=0.4.0->dataclasses_json->llama_index) (1.0.0)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.8/site-packages (0.0.129)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.venv/lib/python3.8/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.8/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in ./.venv/lib/python3.8/site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.8/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.8/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.venv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.venv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.venv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.8/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.8/site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.8/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index\n",
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fuX_I816UT6q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get(\"APIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmCP_UhPUzqj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "from llama_index import GPTSimpleVectorIndex, download_loader, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhizT3ToZdm9",
    "outputId": "2481d14e-9c31-4b54-ae86-a194f04f749c"
   },
   "outputs": [],
   "source": [
    "response = index.query(\"What are some of the sparks of AGI that large language models are exhibiting?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Y04PsZ9fbTX",
    "outputId": "f82bc2f4-9d20-4540-f5bc-a661852a145e"
   },
   "outputs": [],
   "source": [
    "response = index.query(\"What does memory augmented mean?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-Gk9MNS0a1UJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3586 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(llm_predictor\u001b[38;5;241m=\u001b[39mllm_predictor)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# build index\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mGPTKeywordTableIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/llama_index/indices/base.py:98\u001b[0m, in \u001b[0;36mBaseGPTIndex.from_documents\u001b[0;34m(cls, documents, docstore, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     96\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mget_doc_hash())\n\u001b[0;32m---> 98\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    101\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    102\u001b[0m     docstore\u001b[38;5;241m=\u001b[39mdocstore,\n\u001b[1;32m    103\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    105\u001b[0m )\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/llama_index/node_parser/simple.py:44\u001b[0m, in \u001b[0;36mSimpleNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, include_extra_info)\u001b[0m\n\u001b[1;32m     42\u001b[0m all_nodes: List[Node] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 44\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43mget_nodes_from_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_splitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_extra_info\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     all_nodes\u001b[38;5;241m.\u001b[39mextend(nodes)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_nodes\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/llama_index/node_parser/node_utils.py:49\u001b[0m, in \u001b[0;36mget_nodes_from_document\u001b[0;34m(document, text_splitter, include_extra_info)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nodes_from_document\u001b[39m(\n\u001b[1;32m     44\u001b[0m     document: BaseDocument,\n\u001b[1;32m     45\u001b[0m     text_splitter: TextSplitter,\n\u001b[1;32m     46\u001b[0m     include_extra_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Node]:\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add document to index.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     text_splits \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_splits_from_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_extra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     nodes: List[Node] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m     index_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/llama_index/node_parser/node_utils.py:30\u001b[0m, in \u001b[0;36mget_text_splits_from_document\u001b[0;34m(document, text_splitter, include_extra_info)\u001b[0m\n\u001b[1;32m     27\u001b[0m text_splits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_splitter, TokenTextSplitter):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# use this to extract extra information about the chunks\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     text_splits \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text_with_overlaps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_info_str\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minclude_extra_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     text_chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(\n\u001b[1;32m     36\u001b[0m         document\u001b[38;5;241m.\u001b[39mget_text(),\n\u001b[1;32m     37\u001b[0m     )\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/llama_index/langchain_helpers/text_splitter.py:189\u001b[0m, in \u001b[0;36mTokenTextSplitter.split_text_with_overlaps\u001b[0;34m(self, text, extra_info_str)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# 2. Shrink the current_doc (from the front) until it is gets smaller\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# than the overlap size\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# NOTE: because counting tokens individually is an imperfect\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# proxy (but much faster proxy) for the total number of tokens consumed,\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# we need to enforce that start_idx <= cur_idx, otherwise\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# start_idx has a chance of going out of bounds.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cur_total \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_overlap \u001b[38;5;129;01mand\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m cur_idx:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# # call tokenizer on entire overlap\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# cur_total = self.tokenizer()\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     cur_num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    190\u001b[0m     cur_total \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m cur_num_tokens\n\u001b[1;32m    191\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/llama_index/utils.py:52\u001b[0m, in \u001b[0;36mGlobalsHelper.tokenizer.<locals>.tokenizer_fn\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer_fn\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2530\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2636\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2617\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2618\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2633\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2634\u001b[0m     )\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2709\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2700\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2701\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2702\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2707\u001b[0m )\n\u001b[0;32m-> 2709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:176\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:526\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n\u001b[1;32m    523\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m BatchEncoding(\n\u001b[1;32m    524\u001b[0m         {\n\u001b[1;32m    525\u001b[0m             key: value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m--> 526\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbatched_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m         },\n\u001b[1;32m    528\u001b[0m         batched_output\u001b[38;5;241m.\u001b[39mencodings,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(batched_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_length, verbose)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batched_output\n",
      "File \u001b[0;32m~/ECS/github/chatbot-for-documents/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup your LLM\n",
    "\n",
    "from llama_index import (\n",
    "    GPTKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ")\n",
    "from langchain import OpenAI\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "# build index\n",
    "index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pagpwrcMdtsC",
    "outputId": "db08e7b8-544e-4d26-ac01-6dc8a75045aa"
   },
   "outputs": [],
   "source": [
    "response = index.query(\"What are some of the sparks of AGI that large language models are exhibiting?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYEvjBOUfudU",
    "outputId": "5a55ac8d-08a0-4ad5-81d5-ed9686e4b173"
   },
   "outputs": [],
   "source": [
    "response = index.query(\"What does memory augmented mean?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_abcteoegRb5"
   },
   "outputs": [],
   "source": [
    "index.save_to_disk('./index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPxoDL0Xg5Az",
    "outputId": "d09b5819-43be-4491-cf0f-438b80b591b9"
   },
   "outputs": [],
   "source": [
    "response = index.query(\"Summarize the Sparks of AGI paper\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8zzUchDlK4c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-rhB5UslQj7"
   },
   "outputs": [],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(\"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\", index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
