{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Knowledge Chatbot w/ LlamaIndex\n",
        "By Liam Ottley\n",
        "\n",
        "YouTube: https://www.youtube.com/@LiamOttley\n",
        "\n",
        "Github: https://github.com/wombyz/custom-knowledge-chatbot/blob/main/custom-knowledge-chatbot/Custom%20Knowledge%20Chatbot.ipynb\n",
        "\n",
        "LlamaIndex: https://gpt-index.readthedocs.io/en/latest/index.html\n",
        "\n",
        "## Customization\n",
        "Using the example above with fixes from LlamaIndex documentation, this notebook will index the documents in the data directory and allow you to query the index through a chatbot interface\n",
        "\n",
        "## Installation\n",
        "*   Create data directory\n",
        "*   Upload documents to data \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8JUYIIBrTw9R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpeyL1-qlJGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index\n",
        "!pip install langchain\n"
      ],
      "metadata": {
        "id": "FmfuhnUdT-PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "fuX_I816UT6q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load you data into 'Documents' a custom type by LlamaIndex\n",
        "from llama_index import GPTSimpleVectorIndex, download_loader, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "index = GPTSimpleVectorIndex.from_documents(documents)\n"
      ],
      "metadata": {
        "id": "jmCP_UhPUzqj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"What are some of the sparks of AGI that large language models are exhibiting?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhizT3ToZdm9",
        "outputId": "2481d14e-9c31-4b54-ae86-a194f04f749c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Large language models are exhibiting sparks of AGI by being able to generate sequences with recurrent neural networks, scale language modeling with pathways, solve math word problems, generate code infilling and synthesis, measure massive multitask language understanding, measure mathematical problem solving, reduce activation recomputation in large transformer models, quantify social biases in contextual word representations, scale instruction-finetuned language models, training language models to follow instructions with human feedback, improve language understanding by generative pre-training, and demonstrate that language models are unsupervised multitask learners.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"What does memory augmented mean?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y04PsZ9fbTX",
        "outputId": "f82bc2f4-9d20-4540-f5bc-a661852a145e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Memory augmented means that a language model is augmented with an external memory, such as an associative read-write memory, to enable it to process arbitrarily large inputs and potentially simulate any algorithm. This memory is typically a Turing machine, which consists of a finite set of states, a finite set of tape symbols, a blank symbol, a start state, a set of halting (state, symbol) pairs, and a finite set of transition rules that specify the operation of the machine in each compute cycle. The tape memory is initialized with a finite number of non-blank symbols and the tape head can access a single tape location and move one location left or right in each compute cycle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup your LLM\n",
        "\n",
        "from llama_index import (\n",
        "    GPTKeywordTableIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    LLMPredictor,\n",
        "    ServiceContext\n",
        ")\n",
        "from langchain import OpenAI\n",
        "\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "\n",
        "# define LLM\n",
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "\n",
        "# build index\n",
        "index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "-Gk9MNS0a1UJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"What are some of the sparks of AGI that large language models are exhibiting?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pagpwrcMdtsC",
        "outputId": "db08e7b8-544e-4d26-ac01-6dc8a75045aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Some of the sparks of AGI that large language models are exhibiting are the ability to reason, plan, and create. Additionally, they exhibit general and flexible intelligence, which challenges our understanding of learning and cognition.\n",
            "\n",
            "One of the ways in which large language models are exhibiting these sparks of AGI is through their ability to learn from vast amounts of data. This allows them to gain a deep understanding of the world and how it works, which in turn allows them to reason, plan, and create in ways that are far beyond what current AI systems are capable of.\n",
            "\n",
            "For example, large language models have been shown to be able to complete complex tasks that require natural language understanding and extensive command line use. This demonstrates their ability to reason and plan, as well as their general intelligence. Additionally, their ability to learn from vast amounts of data allows them to exhibit flexible intelligence, which challenges our understanding of learning and cognition.\n",
            "\n",
            "Another way in which large language models are exhibiting these sparks of AGI is through their ability to generate text, from news articles to social media posts. This demonstrates their ability to create and communicate in ways that are far beyond what current AI systems are capable of.\n",
            "\n",
            "Additionally, large language models have also been shown to be able\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"What does memory augmented mean?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYEvjBOUfudU",
        "outputId": "5a55ac8d-08a0-4ad5-81d5-ed9686e4b173"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Memory augmented means that the system is able to remember and store information in its memory. This allows the system to learn and improve its performance over time. The additional context provides examples of how this might be used in different fields, such as in large language models or in formal computations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.save_to_disk('./index.json')"
      ],
      "metadata": {
        "id": "_abcteoegRb5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(\"Summarize the Sparks of AGI paper\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPxoDL0Xg5Az",
        "outputId": "d09b5819-43be-4491-cf0f-438b80b591b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The paper explores the potential of GPT-4 to attain a form of general intelligence, demonstrating its core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform. While the model still has some limitations, the authors believe that it shows great promise for future development of artificial general intelligence.\n",
            "\n",
            "In addition, the paper discusses the fundamental questions of why and how GPT-4 achieves such remarkable intelligence. It is suggested that further research is needed to understand the capabilities of LLMs, in order to make progress in the development of artificial general intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, api_key, index):\n",
        "        self.index = index\n",
        "        openai.api_key = api_key\n",
        "        self.chat_history = []\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
        "        prompt += f\"\\nUser: {user_input}\"\n",
        "        response = index.query(user_input)\n",
        "\n",
        "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
        "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        self.chat_history.append(message)\n",
        "        return message\n",
        "    \n",
        "    def load_chat_history(self, filename):\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                self.chat_history = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    def save_chat_history(self, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.chat_history, f)"
      ],
      "metadata": {
        "id": "f8zzUchDlK4c"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Swap out your index below for whatever knowledge base you want\n",
        "bot = Chatbot(\"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\", index=index)\n",
        "bot.load_chat_history(\"chat_history.json\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        bot.save_chat_history(\"chat_history.json\")\n",
        "        break\n",
        "    response = bot.generate_response(user_input)\n",
        "    print(f\"Bot: {response['content']}\")"
      ],
      "metadata": {
        "id": "2-rhB5UslQj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}